{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T2718/AI_Make_Image/blob/main/AI_Make_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QQD6GeyqR0cJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSZ_ciIF64TD",
        "outputId": "49683da0-8c6a-44a7-b15e-8199e1d64a7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diffusers was imported\n",
            "torch was imported\n"
          ]
        }
      ],
      "source": [
        "#　画像出力のディレクトリ\n",
        "\n",
        "# 画像のファイル名\n",
        "import re\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "\n",
        "if \"diffusers\" in sys.modules:\n",
        "  print(\"diffusers was imported\")\n",
        "else:\n",
        "  print(\"diffusers is not imported\")\n",
        "  !pip install diffusers==0.8.0 transformers scipy ftfy\n",
        "\n",
        "if \"torch\" in sys.modules:\n",
        "  print(\"torch was imported\")\n",
        "else:\n",
        "  print(\"torch is not imported\")\n",
        "  !pip install torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "height_num = 640\n",
        "width_num = 360\n",
        "\n",
        "model_id = \"\"\n",
        "\n",
        "while True:\n",
        "  setting = input('Setting用(無入力でいい)')\n",
        "  if setting == \"\":\n",
        "    break\n",
        "  if setting == \"model_id\":\n",
        "    model_id = input('kind')\n",
        "\n",
        "\n",
        "prompt = input('Prompt:')\n",
        "n_prompt = input('Negative_prompt:')\n",
        "strong = int(input('Strong:'))\n",
        "kind = input('画像を使用するか(T or F)\\n(Tの場合は、outputfileディレクトリのあるところと同じ階層に、\\ninput.pngで保存してください。)\\n→:')\n",
        "if(kind == 'T'):\n",
        "  image_strength = float(input('image_Strength:'))\n",
        "  init_image = Image.open('input.png').convert('RGB')\n",
        "  size_max_multi = max(init_image.width/width_num,init_image.height/height_num)\n",
        "  init_image = init_image.resize((int(init_image.width/size_max_multi),int(init_image.height/size_max_multi)))\n",
        "  print('Multi:'+str(size_max_multi))\n",
        "  #init_image\n",
        "  #print(init_image)\n",
        "\n",
        "\"\"\"if 'model' in locals():\n",
        "  print('Model was defined')\n",
        "else:\n",
        "  print('Model is undefined')\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num = int(input('Amount:'))\n",
        "image_num_onetime = int(input('Onetime Amount:'))\n",
        "\n",
        "\n",
        "\n",
        "if 'pre_kind' not in locals():\n",
        "  pre_kind = ''\n",
        "\n",
        "\n",
        "\n",
        "model_id = 'Oscarguid/DivineEleganceMixV9'\n",
        "access_tokens = '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'\n",
        "\n",
        "if pre_kind != kind and kind == 'T':\n",
        "  from diffusers import StableDiffusionPipeline,StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n",
        "  import torch\n",
        "\n",
        "  # アクセストークンの設定\n",
        "  #access_tokens=\"\" # @param {type:\"string\"}\n",
        "\n",
        "  # モデルのインスタンス化\n",
        "  # stablediffusionapi/anything-v5\n",
        "  # Oscarguid/DivineEleganceMixV9\n",
        "  # andite/pastel-mix\n",
        "  #model_id = 'Oscarguid/DivineEleganceMixV9'\n",
        "\n",
        "\n",
        "\n",
        "  model = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_auth_token=access_tokens, safety_checker=None)\n",
        "  model.scheduler = DPMSolverMultistepScheduler.from_config(model.scheduler.config)\n",
        "  model.to(\"cuda\")\n",
        "  print(model)\n",
        "elif pre_kind != kind and kind == 'F':\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  import torch\n",
        "  model = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=access_tokens)\n",
        "  model.to('cuda')\n",
        "\n",
        "\n",
        "pre_kind = kind\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "filename = re.sub(r'[\\\\/:*?\"<>|,]+', '', prompt).replace(' ','_')\n",
        "if len(filename) >= 20:\n",
        "  filename = filename[0:19]\n",
        "try:\n",
        "  try:\n",
        "    os.mkdir('outputfile')\n",
        "  except:\n",
        "    pass\n",
        "  os.mkdir('outputfile/'+filename)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "# 画像数\n",
        "#num\n",
        "\n",
        "def null_safety(images, **kwargs):\n",
        "    #print('NSFW')\n",
        "    return images, False\n",
        "\n",
        "\n",
        "try:\n",
        "  model.safety_checker = null_safety\n",
        "except:\n",
        "  print('error')\n",
        "\n",
        "\n",
        "for i in range(num):\n",
        "  # モデルにpromptを入力し画像生成\n",
        "  print(str(i+1)+'枚目')\n",
        "  if kind == 'T':\n",
        "    #print(init_image)\n",
        "    #print(model)\n",
        "    #if not isinstance(model, StableDiffusionImg2ImgPipeline):\n",
        "    #  print(\"Error: `model` is not of type `StableDiffusionImg2ImgPipeline`. Please make sure you have loaded the correct model.\")\n",
        "    \"\"\"if \"image\" not in model.__call__.__kwdefaults__:\n",
        "      print(\"Error: The `image` argument is missing from the `model` function. Please make sure you are using the correct version of the `diffusers` library.\")\n",
        "    \"\"\"\n",
        "    image = model(prompt,height=height_num,width=width_num,negative_prompt=n_prompt,init_image=init_image,strength=image_strength,num_inference_steps=strong-1,num_images_per_prompt=image_num_onetime)\n",
        "  else:\n",
        "    image = model(prompt,height=height_num,width=width_num,negative_prompt=n_prompt,num_inference_steps=strong-1,num_images_per_prompt=image_num_onetime)\n",
        "  #print(image.keys())\n",
        "  image_all = image['images']\n",
        "  print(str(len(image_all))+'枚')\n",
        "  count = 0\n",
        "  filename2 = f'_{i+1:02}_'\n",
        "  try:\n",
        "    os.mkdir(f\"outputfile/{filename}/{filename2}\")\n",
        "  except:\n",
        "    pass\n",
        "  for k in image_all:\n",
        "    count += 1\n",
        "    image = k\n",
        "  # 保存\n",
        "\n",
        "    image.save(f\"outputfile/{filename}/{filename2}/_{count:02}_.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('outputfile')"
      ],
      "metadata": {
        "id": "K9DJaEHAEPzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "metadata": {
        "id": "sz7dY1z-xcAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}